{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c696178",
   "metadata": {},
   "source": [
    "# Multi-Product Supermarket Inventory Management with pi_optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a5c241",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to use `pi_optimal` to optimize inventory management decisions for multiple product categories in a supermarket setting. We'll focus on finding the optimal ordering strategy for different product types: bakery items, dairy products, and fresh produce, each with different shelf lives, profit margins, and demand patterns.\n",
    "\n",
    "The key challenges in multi-product inventory management include:\n",
    "- **Stockouts**: Not having enough inventory leads to lost sales and unhappy customers\n",
    "- **Waste**: Ordering too much leads to expired products and financial losses\n",
    "- **Holding Costs**: Maintaining inventory incurs costs (refrigeration, space, handling)\n",
    "- **Product Interactions**: Demand for one product may influence demand for others\n",
    "\n",
    "Using reinforcement learning with `pi_optimal`, we'll develop an ordering strategy that balances these competing objectives across multiple product types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aec873",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Concepts\n",
    "\n",
    "Before diving into the implementation, let's understand key reinforcement learning terms in the context of multi-product inventory management:\n",
    "\n",
    "- **State**: Represents the current situation of our inventory system, including:\n",
    "  - Current stock levels for each product category\n",
    "  - Demand history for each product category\n",
    "  - Day of week (affects demand patterns)\n",
    "  - Recent waste and stockouts by product type\n",
    "\n",
    "- **Action**: The decisions we make - specifically how many units to order for each product category (bakery, dairy, fresh produce).\n",
    "\n",
    "- **Reward**: A numerical signal indicating how good or bad our decisions were. For multi-product inventory, rewards might consider:\n",
    "  - Overall profit across all product categories\n",
    "  - Category-specific performance\n",
    "  - Balance between waste minimization and stockout prevention\n",
    "\n",
    "- **Policy**: A strategy that determines what ordering decisions to make for each product type given the current state.\n",
    "\n",
    "- **Offline RL**: Learning from historical inventory data rather than through direct trial-and-error. This approach is ideal for inventory optimization where experimentation can be costly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23e140c",
   "metadata": {},
   "source": [
    "---\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Configuration](#setup-and-configuration)\n",
    "2. [Understanding the Multi-Product Environment](#understanding-the-multi-product-environment)\n",
    "3. [Data Collection](#data-collection)\n",
    "   - [Simulating Random Ordering Strategies](#simulating-random-ordering-strategies)\n",
    "   - [Exploring Collected Data](#exploring-collected-data)\n",
    "4. [Defining Custom Reward Functions](#defining-custom-reward-functions)\n",
    "5. [Training with pi_optimal](#training-with-pi_optimal)\n",
    "   - [Dataset Preparation](#dataset-preparation)\n",
    "   - [Agent Configuration](#agent-configuration)\n",
    "   - [Training the Agent](#training-the-agent)\n",
    "6. [Evaluating Performance](#evaluating-performance)\n",
    "   - [Policy Visualization](#policy-visualization)\n",
    "7. [Conclusion and Next Steps](#conclusion-&-next-steps)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26457bf7",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, let's import the necessary libraries and load our multi-product supermarket environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0455485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "import gymnasium as gym\n",
    "import pi_optimal as po\n",
    "from supermarket_env import MultiProductSupermarketEnv\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = [14, 8]\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981e6468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multi-product supermarket environment with default settings\n",
    "multi_product_env = MultiProductSupermarketEnv()\n",
    "\n",
    "# Print the product names in the multi-product environment\n",
    "print(f\"Products available in the multi-product environment: {multi_product_env.product_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c306be2",
   "metadata": {},
   "source": [
    "## Understanding the Multi-Product Environment\n",
    "\n",
    "Let's explore the properties of our multi-product environment to better understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d91615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the observation and action spaces\n",
    "print(\"Observation space:\", multi_product_env.observation_space)\n",
    "print(\"Action space:\", multi_product_env.action_space)\n",
    "\n",
    "# Get initial observation and info\n",
    "initial_obs, info = multi_product_env.reset(seed=42)\n",
    "\n",
    "# Print observation keys to understand what information is available\n",
    "print(\"\\nObservation keys:\")\n",
    "for key in initial_obs.keys():\n",
    "    print(f\"  - {key}: {type(initial_obs[key])}\")\n",
    "\n",
    "# Display sample info dictionary\n",
    "print(\"\\nInfo keys:\")\n",
    "for key in info.keys():\n",
    "    print(f\"  - {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e681ba",
   "metadata": {},
   "source": [
    "Let's examine the product characteristics to understand their differences:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f731a6e8",
   "metadata": {},
   "source": [
    "### Dataset Features\n",
    "\n",
    "The `product_configs` dictionary defines how each product behaves in the environment. For each product (e.g. `\"fresh_produce\"`, `\"dairy\"`, `\"bakery\"`), you’ll find:\n",
    "\n",
    "| **Key**            | **Meaning**                                                                                             |\n",
    "|--------------------|---------------------------------------------------------------------------------------------------------|\n",
    "| `max_inventory`    | Maximum units you can stock at once.                                                                    |\n",
    "| `shelf_life`       | Number of days before unsold items spoil.                                                               |\n",
    "| `purchase_cost`    | Cost per unit when ordering from supplier.                                                               |\n",
    "| `selling_price`    | Revenue per unit sold to customers.                                                                     |\n",
    "| `holding_cost`     | Cost per unit per day for storage (e.g. refrigeration, shelf space).                                     |\n",
    "| `stockout_cost`    | Penalty per unit of unmet demand (e.g. lost sale or backorder cost).                                     |\n",
    "| `waste_cost`       | Penalty per spoiled unit at end of its shelf life.                                                      |\n",
    "| `demand_mean`      | Average daily demand (in units) before noise/weekday adjustments.                                        |\n",
    "| `demand_std`       | Standard deviation of daily demand (for stochastic sampling).                                           |\n",
    "| `weekday_factors`  | List of 7 multipliers (Mon→Sun) that scale demand to capture weekday/weekend effects.                     |\n",
    "\n",
    "Additionally, the shared `common_config` contains:\n",
    "\n",
    "- `episode_length`: number of days per simulation episode (default 30).  \n",
    "- `seed`: random seed for reproducibility (if set).  \n",
    "\n",
    "By tuning these parameters, you can model everything from highly perishable produce to long‑lasting goods, and explore how your reward functions and ordering policies must adapt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc2c7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table showing the characteristics of each product\n",
    "product_data = []\n",
    "\n",
    "for product in multi_product_env.product_names:\n",
    "    product_info = {\n",
    "        'Product': product,\n",
    "        'Shelf Life (days)': multi_product_env.product_configs[product]['shelf_life'],\n",
    "        'Purchase Cost': multi_product_env.product_configs[product]['purchase_cost'],\n",
    "        'Selling Price': multi_product_env.product_configs[product]['selling_price'],\n",
    "        'Storage Cost': multi_product_env.product_configs[product]['holding_cost'],\n",
    "        'Profit Margin': multi_product_env.product_configs[product]['selling_price'] - \n",
    "                        multi_product_env.product_configs[product]['purchase_cost']\n",
    "    }\n",
    "    product_data.append(product_info)\n",
    "\n",
    "# Create a DataFrame to display\n",
    "product_df = pd.DataFrame(product_data)\n",
    "product_df.sort_values('Shelf Life (days)')\n",
    "product_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a09856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5bde3",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "To train our RL agent, we need to collect data on different ordering strategies for multiple products. We'll implement and simulate various ordering policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49db3ef9",
   "metadata": {},
   "source": [
    "### Simulating Random Ordering Strategies\n",
    "\n",
    "Let's implement a function to collect data with different ordering strategies for multiple products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5238f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_multi_product_data(env, n_episodes=50, max_steps=30, ordering_strategies=None, random_seed=42):\n",
    "    \"\"\"\n",
    "    Collect data by running multiple episodes with different ordering strategies\n",
    "    for multiple products.\n",
    "    \"\"\"\n",
    "    # Ensure reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Define default strategies if none provided\n",
    "    if ordering_strategies is None:\n",
    "        # -- Multi-product strategies --\n",
    "        def random_small_order(obs, info):\n",
    "            return tuple(np.array([np.random.randint(0, 21)]) for _ in env.product_names)\n",
    "        \n",
    "        def random_medium_order(obs, info):\n",
    "            return tuple(np.array([np.random.randint(10, 31)]) for _ in env.product_names)\n",
    "        \n",
    "        def random_large_order(obs, info):\n",
    "            return tuple(np.array([np.random.randint(20, 41)]) for _ in env.product_names)\n",
    "        \n",
    "        def replenish_to_target(obs, info, target=30):\n",
    "            actions = []\n",
    "            for pname in env.product_names:\n",
    "                inv = np.sum(obs[f'{pname}_inventory'])\n",
    "                actions.append(np.array([max(0, target - inv)]))\n",
    "            return tuple(actions)\n",
    "        \n",
    "        def order_based_on_demand(obs, info):\n",
    "            actions = []\n",
    "            for pname in env.product_names:\n",
    "                dh = obs[f'{pname}_demand_history']\n",
    "                avg_d = np.mean(dh)\n",
    "                actions.append(np.array([int(avg_d + 5)]))\n",
    "            return tuple(actions)\n",
    "        \n",
    "        def product_specific_strategy(obs, info):\n",
    "            actions = []\n",
    "            # For bakery (shorter shelf life) - order close to immediate demand\n",
    "            bakery_demand = np.mean(obs['bakery_demand_history'][-3:])\n",
    "            actions.append(np.array([int(bakery_demand * 1.1)]))\n",
    "            \n",
    "            # For dairy (medium shelf life) - maintain moderate stock\n",
    "            dairy_inv = np.sum(obs['dairy_inventory'])\n",
    "            actions.append(np.array([max(0, 25 - dairy_inv)]))\n",
    "            \n",
    "            # For fresh produce - balance based on recent sales\n",
    "            fp_demand = np.mean(obs['fresh_produce_demand_history'])\n",
    "            fp_inv = np.sum(obs['fresh_produce_inventory'])\n",
    "            actions.append(np.array([max(0, int(fp_demand * 1.5) - fp_inv)]))\n",
    "            \n",
    "            return tuple(actions)\n",
    "        \n",
    "        ordering_strategies = [\n",
    "            random_small_order,\n",
    "            random_medium_order,\n",
    "            random_large_order,\n",
    "            replenish_to_target,\n",
    "            order_based_on_demand,\n",
    "            product_specific_strategy\n",
    "        ]\n",
    "\n",
    "    all_episodes = []\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        strat = np.random.choice(ordering_strategies)\n",
    "        strat_name = strat.__name__\n",
    "\n",
    "        obs, info = env.reset(seed=random_seed + ep)\n",
    "        for step in range(max_steps):\n",
    "            action = strat(obs, info)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        df = env.get_episode_history()\n",
    "        df['episode'] = ep\n",
    "        df['strategy'] = strat_name\n",
    "        all_episodes.append(df)\n",
    "\n",
    "    return pd.concat(all_episodes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a847e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect training data\n",
    "train_df = collect_multi_product_data(multi_product_env, n_episodes=60, max_steps=30)\n",
    "\n",
    "# Collect test data\n",
    "test_df = collect_multi_product_data(multi_product_env, n_episodes=10, max_steps=20)\n",
    "\n",
    "# Display the first few rows\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a17c76",
   "metadata": {},
   "source": [
    "### Exploring Collected Data\n",
    "\n",
    "Let's visualize the data to understand the relationships between different metrics for each product category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee377208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize key metrics by product category with generic columns\n",
    "def summarize_by_product(df):\n",
    "    product_metrics = []\n",
    "    \n",
    "    for product in multi_product_env.product_names:\n",
    "        # Extract metrics for this product but use generic column names\n",
    "        metrics = {\n",
    "            'product': product,\n",
    "            'order': df[f'{product}_order'].mean(),\n",
    "            'demand': df[f'{product}_demand'].mean(),\n",
    "            'inventory': df[f'{product}_inventory'].mean(),\n",
    "            'waste': df[f'{product}_waste'].mean(),\n",
    "            'stockouts': df[f'{product}_stockouts'].mean(),\n",
    "            'profit': df[f'{product}_profit'].mean()\n",
    "        }\n",
    "        product_metrics.append(metrics)\n",
    "    \n",
    "    # Create DataFrame from list of dictionaries\n",
    "    return pd.DataFrame(product_metrics).set_index('product')\n",
    "\n",
    "# Calculate summary by product\n",
    "product_summary = summarize_by_product(train_df)\n",
    "product_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210d026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize order vs. waste vs. stockouts for each product\n",
    "fig, axes = plt.subplots(1, len(multi_product_env.product_names), figsize=(18, 6))\n",
    "\n",
    "for i, product in enumerate(multi_product_env.product_names):\n",
    "    ax = axes[i]\n",
    "    scatter = ax.scatter(\n",
    "        train_df[f'{product}_order'], \n",
    "        train_df[f'{product}_waste'],\n",
    "        c=train_df[f'{product}_stockouts'],\n",
    "        cmap='viridis',\n",
    "        alpha=0.6\n",
    "    )\n",
    "    ax.set_title(f'{product.capitalize()}')\n",
    "    ax.set_xlabel('Order Quantity')\n",
    "    ax.set_ylabel('Waste')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add a colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('Stockouts')\n",
    "\n",
    "plt.suptitle('Order vs. Waste vs. Stockouts by Product Category', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e7b6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze profit distribution by strategy\n",
    "strategy_performance = train_df.groupby('strategy').agg({\n",
    "    'bakery_profit': 'mean',\n",
    "    'dairy_profit': 'mean',\n",
    "    'fresh_produce_profit': 'mean',\n",
    "    'total_profit': 'mean',\n",
    "    'total_waste': 'mean',\n",
    "    'total_stockouts': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Sort by total profit\n",
    "strategy_performance = strategy_performance.sort_values('total_profit', ascending=False)\n",
    "strategy_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ac6d2e",
   "metadata": {},
   "source": [
    "## Defining Custom Reward Functions\n",
    "\n",
    "Let's define different reward functions to train our agent for different objectives in multi-product inventory management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ef8ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profit_oriented_reward(row):\n",
    "    \"\"\"\n",
    "    Calculates a reward focused on maximizing total profit across all products.\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series containing metrics for a single step\n",
    "    \n",
    "    Returns:\n",
    "        The calculated reward value\n",
    "    \"\"\"\n",
    "    # Simply return the total profit\n",
    "    return row['total_profit']\n",
    "\n",
    "\n",
    "# Apply the profit-oriented reward to our training data\n",
    "train_df['reward'] = train_df.apply(profit_oriented_reward, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b02141",
   "metadata": {},
   "source": [
    "Below this heading is where **you** get to shape the agent’s behavior by defining your own reward functions. Think of a reward function as the “business objective” you hand to the learner—anything you can express in terms of profit, cost, service or quality can become part of your reward signal.\n",
    "\n",
    "**What to try**  \n",
    "- **Penalty terms**: discourage stockouts, spoilage, large order swings, or over‑holding.  \n",
    "- **Bonus terms**: reward freshness (selling early), high service levels, or hitting promotional targets.  \n",
    "- **Multi‑objective trade‑offs**: blend profit with waste, customer satisfaction, or inventory turns via weighted sums.  \n",
    "- **Non‑linear incentives**: use thresholds, piecewise rewards, or saturating functions to prioritize certain ranges.\n",
    "\n",
    "**How to add your function**  \n",
    "1. Define a Python function that takes a single row of your simulation log (`train_df`) and returns a scalar reward.  \n",
    "2. Register it by assigning to `train_df['reward'] = train_df.apply(your_reward, axis=1)`.  \n",
    "3. Re‑train your agent and observe how its ordering policy shifts under your design!\n",
    "\n",
    "Feel free to experiment wildly—seeing how small changes in your reward shape can lead to very different ordering strategies is the heart of reward‑function engineering. Have fun, and don’t forget to visualize profit vs. waste vs. stockouts afterward!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dcc170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def your_custom_reward_function(row):\n",
    "    \"\"\"\n",
    "    Custom reward function that considers multiple factors.\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas Series containing metrics for a single step\n",
    "    \n",
    "    Returns:\n",
    "        The calculated reward value\n",
    "    \"\"\"\n",
    "    # Example: penalize waste and stockouts, reward profit\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50ede1",
   "metadata": {},
   "source": [
    "## Training with Pi_Optimal\n",
    "\n",
    "Now let's prepare the dataset and train pi_optimal agents for multi-product inventory optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3cba57",
   "metadata": {},
   "source": [
    "### Dataset Preparation\n",
    "\n",
    "Let's prepare the dataset for Pi_Optimal, focusing on features relevant to multi-product inventory management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8138e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state_columns = ['day_of_week']\n",
    "\n",
    "# Add product-specific state columns\n",
    "for product in multi_product_env.product_names:\n",
    "    state_columns.extend([\n",
    "        f'{product}_inventory',  # Current inventory level\n",
    "        f'{product}_waste',      # Recent waste\n",
    "        f'{product}_stockouts',  # Recent stockouts\n",
    "        f'{product}_demand'      # Recent demand\n",
    "    ])\n",
    "\n",
    "# Define action columns - one for each product\n",
    "action_columns = [f'{product}_order' for product in multi_product_env.product_names]\n",
    "\n",
    "lookback = 7 # Number of days to look back for state features \n",
    "reward_column = 'reward' \n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = po.datasets.timeseries_dataset.TimeseriesDataset(\n",
    "    df=train_df,\n",
    "    lookback_timesteps=lookback,\n",
    "    unit_index='episode',         # Each episode is a separate unit\n",
    "    timestep_column='day',        # Day is our timestep\n",
    "    reward_column=reward_column,  # Use the specified reward\n",
    "    state_columns=state_columns,  # State features\n",
    "    action_columns=action_columns # Action features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f49c2f3",
   "metadata": {},
   "source": [
    "### Agent Configuration\n",
    "\n",
    "Let's set up the Pi_Optimal agent configuration suitable for multi-product inventory management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930dfff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model configuration for multi-product agents\n",
    "# We'll use random forest models which work well for this type of problem\n",
    "model_config = [\n",
    "    {\n",
    "        \"model_type\": \"RandomForest\",\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 150,\n",
    "            \"max_depth\": None,\n",
    "            \"min_samples_leaf\": 2\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"model_type\": \"RandomForest\",\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 150,\n",
    "            \"max_depth\": None,\n",
    "            \"min_samples_leaf\": 2\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6f49cc",
   "metadata": {},
   "source": [
    "### Training the Agent\n",
    "\n",
    "Now let's train agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d4d374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pi_optimal.agents.agent import Agent\n",
    "\n",
    "# Train the profit-oriented agent\n",
    "profit_agent = Agent()\n",
    "profit_agent.train(train_dataset, model_config=model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe4ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_agent.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c89387",
   "metadata": {},
   "source": [
    "## Evaluating Performance\n",
    "\n",
    "Now let's evaluate how each agent performs with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e750299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply reward functions to test data\n",
    "test_df['reward'] = test_df.apply(profit_oriented_reward, axis=1)\n",
    "\n",
    "\n",
    "# Create test datasets\n",
    "current_dataset = po.datasets.timeseries_dataset.TimeseriesDataset(\n",
    "                                    df=test_df,\n",
    "                                    dataset_config=profit_agent.dataset_config,\n",
    "                                    train_processors=False,\n",
    "                                    is_inference=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict optimal actions using each agent\n",
    "best_actions = profit_agent.predict(current_dataset, horizon=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a03352",
   "metadata": {},
   "source": [
    "### Policy Visualization\n",
    "\n",
    "Let's visualize the predicted actions from each agent to understand their decision-making patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(best_actions)):\n",
    "    print(f\"Timestep {i}:\")    \n",
    "    for j, product in enumerate(multi_product_env.product_names):\n",
    "        print(f\"  - {product}: {best_actions[i][j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b60c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pi_optimal.utils.trajectory_visualizer import TrajectoryVisualizer\n",
    "\n",
    "trajectory_visualizer = TrajectoryVisualizer(profit_agent, current_dataset, best_actions=best_actions)\n",
    "trajectory_visualizer.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b70149",
   "metadata": {},
   "source": [
    "## Conclusion & Next Steps\n",
    "\n",
    "You’ve now explored how to build and customize a multi‑product inventory RL environment, defined your own reward functions, and trained agents to balance profit, waste and service. Here are some ideas for where to take your sprint next:\n",
    "\n",
    "1. **Extend the Environment**  \n",
    "   - Add order lead‐times or supplier capacity constraints  \n",
    "   - Model volume discounts, spoilage rates that change over time, or dynamic pricing incentives  \n",
    "\n",
    "2. **Advance Your Reward Designs**  \n",
    "   - Incorporate customer satisfaction metrics (e.g. fill‑rate targets)  \n",
    "   - Experiment with non‑linear or thresholded penalties (e.g. steep drop after a stockout count)  \n",
    "\n",
    "3. **Use Advance Models**  \n",
    "   - Add new models to our package to experiment with more complex neural networks\n",
    "\n",
    "4. **Analyze & Share Your Findings**  \n",
    "   - Compare your policies under different business scenarios  \n",
    "   - Visualize trade‑offs between profit, waste and service levels  \n",
    "   - Contribute your best reward functions or policy benchmarks back to the pi_optimal repository  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d456f31",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pi-optimal-QIZaaRTp-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
